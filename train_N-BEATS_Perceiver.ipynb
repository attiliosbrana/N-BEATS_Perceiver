{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eva01/anaconda3/envs/pytorch_2.0_full/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Basic stuff\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Iterator\n",
    "import time, datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "#Add ./perceiver to path\n",
    "import sys\n",
    "sys.path.append('./perceiver')\n",
    "\n",
    "#Math stuff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as t\n",
    "from torch import optim\n",
    "\n",
    "#Custom stuff\n",
    "from utils import Meta\n",
    "from sampler import get_train_batch, get_test_batch\n",
    "\n",
    "#Model\n",
    "from nbeats_perceiver import generic\n",
    "from ranger import Ranger\n",
    "from flatplusanneal import FlatplusAnneal\n",
    "from losses import smape_2_loss, mape_loss, mase_loss, rmsse_loss\n",
    "\n",
    "#Import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Remove warnings\n",
    "from numba.core.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = NumbaPendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the file to download\n",
    "file_url = 'https://osf.io/2pf93/download'\n",
    "\n",
    "# send a GET request to the URL\n",
    "response = requests.get(file_url)\n",
    "\n",
    "# check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # open the file in binary write mode and write the response content to it\n",
    "    with open('./data/binance_dataset_original_20220112.pkl', 'wb') as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "# load the downloaded data from the file\n",
    "data = pickle.load(open('./data/binance_dataset_original_20220112.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    def __init__(self):\n",
    "        #Meta date data\n",
    "        self.date_array_len = 8 \n",
    "        \n",
    "        #Frequency\n",
    "        self.seasonal_pattern = seasonal_pattern\n",
    "        self.lookback = lookback\n",
    "        self.outsample_size = Meta.horizons_map[self.seasonal_pattern]\n",
    "        self.insample_size = self.lookback * self.outsample_size\n",
    "        self.input_size = self.insample_size + self.date_array_len\n",
    "        self.timeseries_frequency = Meta.frequency_map[self.seasonal_pattern]\n",
    "        \n",
    "        #Training params\n",
    "        self.epochs = 5500 \n",
    "        self.retrain_epochs = 9000\n",
    "        self.batch_size = 32 \n",
    "        self.lr_decay_step = self.epochs // 3\n",
    "        self.learning_rate =  0.001\n",
    "\n",
    "        #Options\n",
    "        self.loss_options = ['MASE', 'SMAPE', 'MAPE']\n",
    "        \n",
    "        #stack params: input_size, output_size, stacks, layers, layer_size\n",
    "        self.stacks = 20 \n",
    "        self.layers = 4\n",
    "        self.layer_size = 512\n",
    "        self.stack_params = [self.input_size, self.outsample_size, self.stacks, self.layers, self.layer_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(HyperParameters):\n",
    "    def __init__(self):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.seed = seed\n",
    "        self.trainer_epoch = 0\n",
    "        \n",
    "        #Load data\n",
    "        self.data = data \n",
    "        \n",
    "        #Init model\n",
    "        self.init_model()\n",
    "    \n",
    "    #Model parellelism\n",
    "    def init_model(self):\n",
    "        self.model = generic(self.stack_params)\n",
    "        if t.cuda.device_count() > 1:\n",
    "            self.model = t.nn.parallel.DataParallel(self.model, device_ids=list(range(t.cuda.device_count())))\n",
    "        self.model.to(\"cuda:0\")\n",
    "    \n",
    "    #Transform in tensor\n",
    "    def to_tensor(self, array):\n",
    "        return t.tensor(array, dtype=t.float32).to(\"cuda:0\")\n",
    "        \n",
    "    #Loss step\n",
    "    def loss_step(self, batch):\n",
    "        x, x_mask, y, y_mask = map(self.to_tensor, batch)\n",
    "        y_hat  = self.model(x, x_mask)\n",
    "        loss = self.loss_func(x, self.timeseries_frequency, y_hat, y, y_mask)\n",
    "        return loss\n",
    "    \n",
    "    #Possible loss functions\n",
    "    def loss_fn(self, loss_name):\n",
    "        def loss(x, freq, forecast, target, target_mask):\n",
    "            if loss_name == 'MAPE':\n",
    "                return mape_loss(forecast, target, target_mask)\n",
    "            elif loss_name == 'MASE':\n",
    "                return mase_loss(x, freq, forecast, target, target_mask)\n",
    "            elif loss_name == 'SMAPE':\n",
    "                return smape_2_loss(forecast, target, target_mask)\n",
    "            else:\n",
    "                raise Exception(f'Unknown loss function: {loss_name}')\n",
    "        return loss\n",
    "    \n",
    "    #Batch functions\n",
    "    def train_batch(self):\n",
    "        #get_train_batch for self.data\n",
    "        return get_train_batch(self.data['dates'], self.data['dates_array'], self.data['train_cutoff'],\n",
    "                                 self.data['series'],\n",
    "                                 self.data['start_offsets'], self.data['finish_offsets'],\n",
    "                                 self.data['train_eligible'], self.data['compatibility'],\n",
    "                                 self.insample_size, self.outsample_size, self.batch_size)\n",
    "    \n",
    "    def test_batch(self):\n",
    "        return get_test_batch(self.data['dates'], self.data['dates_array'], self.data['train_cutoff'],\n",
    "                                    self.data['series'],\n",
    "                                    self.data['start_offsets'], self.data['finish_offsets'],\n",
    "                                    self.data['test_eligible'], self.data['compatibility'],\n",
    "                                    self.insample_size, self.outsample_size, self.batch_size)\n",
    "    \n",
    "    #Train\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        batch = self.train_batch()\n",
    "        loss = self.loss_step(batch)\n",
    "        loss.backward()\n",
    "        self.train_loss = loss.detach()\n",
    "        t.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.writer.add_scalar('train_' + self.loss_name, self.train_loss, self.epoch)\n",
    "    \n",
    "    #Test\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        with t.no_grad():\n",
    "            batch = self.test_batch()\n",
    "            x, x_mask, y, y_mask = map(self.to_tensor, batch)\n",
    "            y_hat = self.model(x, x_mask)\n",
    "            self.smape = self.loss_fn('SMAPE')(x, self.timeseries_frequency, y_hat, y, y_mask).cpu().numpy()\n",
    "            self.mase = self.loss_fn('MASE')(x, self.timeseries_frequency, y_hat, y, y_mask).cpu().numpy()\n",
    "            self.mape = self.loss_fn('MAPE')(x, self.timeseries_frequency, y_hat, y, y_mask).cpu().numpy()\n",
    "            \n",
    "    #Save checkpoint\n",
    "    def save_checkpoint(self):\n",
    "        try:\n",
    "            t.save(self.model.module.state_dict(), './model_checkpoints/' + self.run_name + '.pt')\n",
    "        except:\n",
    "            t.save(self.model.state_dict(), './model_checkpoints/' + self.run_name + '.pt')\n",
    "    \n",
    "    #Load checkpoint\n",
    "    def load_checkpoint(self, run_name):\n",
    "        load = t.load('./model_checkpoints/' + run_name + '.pt')\n",
    "        try:\n",
    "            self.model.load_state_dict(load)\n",
    "        except:\n",
    "            self.model.module.load_state_dict(load)\n",
    "        \n",
    "    def log(self):\n",
    "        self.save_checkpoint()\n",
    "        self.writer.add_scalar('SMAPE', self.smape, self.epoch)\n",
    "        self.writer.add_scalar('MASE', self.mase, self.epoch)\n",
    "        self.writer.add_scalar('MAPE', self.mape, self.epoch)\n",
    "    \n",
    "    #Trainer steps\n",
    "    def fit(self, meta_name, seed, loss_name, retrain = False):\n",
    "        \n",
    "        #Initial params\n",
    "        self.meta_name = meta_name\n",
    "        self.seed = seed\n",
    "        self.loss_name = loss_name\n",
    "        \n",
    "        #Start time counter\n",
    "        self.total_t0 = time.time()\n",
    "        \n",
    "        #Setup tensorflow logs\n",
    "        self.run_name = '{}_{}_s{}_l{}_l{}'.format(self.meta_name, self.seasonal_pattern,\n",
    "                                                   str(self.seed), self.lookback, self.loss_name)\n",
    "        self.writer = SummaryWriter('./logs/{}'.format(self.run_name))\n",
    "\n",
    "        #Setup optimizer, earning rate scheduler and loss function\n",
    "        self.optimizer = Ranger(self.model.parameters(), lr = 1e-3)\n",
    "        if not retrain:\n",
    "            self.sched = FlatplusAnneal(self.optimizer, max_iter=self.epochs, step_size=0.7)\n",
    "            epochs = self.epochs\n",
    "            print('Training...')\n",
    "        else:\n",
    "            self.sched = FlatplusAnneal(self.optimizer, max_iter=self.retrain_epochs, step_size=0)\n",
    "            epochs = self.retrain_epochs\n",
    "            print('Retraining...')\n",
    "        self.loss_func = self.loss_fn(self.loss_name)\n",
    "\n",
    "        print('Run name:', self.run_name)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.trainer_epoch += 1\n",
    "            self.epoch = epoch\n",
    "            t0 = time.time()\n",
    "            self.train()\n",
    "            t1 = time.time()\n",
    "            ttime = str(datetime.timedelta(seconds = round(t1 - self.total_t0)))\n",
    "            etime = str(datetime.timedelta(seconds = round(t1 - t0)))\n",
    "            if epoch % 250 == 0:\n",
    "                self.ttime = ttime\n",
    "                self.test()\n",
    "                self.log()\n",
    "                # print(\"Epoch: {}\\tSMAPE: {:.3f}\\tMASE: {:.3f}\\tMAPE: {:.3f}\\tETime: {}\\tTTime: {}\"\n",
    "                 # .format(epoch + 1, self.smape, self.mase, self.mape, etime, ttime))\n",
    "            self.sched.step()\n",
    "\n",
    "        self.ttime = ttime\n",
    "        print('TTime:', ttime)\n",
    "        \n",
    "    def retrain(self, run_name, meta_name, seed, loss_name):\n",
    "        self.load_checkpoint(run_name)\n",
    "        self.fit(meta_name, seed, loss_name, retrain = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get todays date\n",
    "today = datetime.date.today()\n",
    "\n",
    "#Transform it into a string\n",
    "today = str(today)\n",
    "\n",
    "#function to produce the combinations\n",
    "def get_combinations(name, n):\n",
    "    tracking = {}\n",
    "    combinations = []\n",
    "    lookbacks = [2, 3, 4, 5, 6, 7]\n",
    "    losses = ['MAPE', 'MASE', 'SMAPE']\n",
    "\n",
    "    #Get all combinations of lookbacks and losses\n",
    "    for lookback in lookbacks:\n",
    "        for loss in losses:\n",
    "            tracking[str(lookback) + '_' + loss] = {'counts':0}\n",
    "            for i in range(n):\n",
    "                combinations.append([name, lookback, loss, i])\n",
    "\n",
    "    return tracking, combinations \n",
    "\n",
    "tracking, combinations = get_combinations('perc', 3)\n",
    "\n",
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** Start time: 08:40:51\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Training...\n",
      "Run name: perc_Hourly_s6412_l6_lMAPE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eva01/TRADING_BOT/forecasting_model/model/ranger.py:115: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at /opt/conda/conda-bld/pytorch_1669968767843/work/torch/csrc/utils/python_arg_parser.cpp:1488.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTime: 2:18:03\n",
      "*************** Start time: 10:58:57\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Training...\n",
      "Run name: perc_Hourly_s17271_l6_lMAPE\n",
      "TTime: 2:20:08\n",
      "*************** Start time: 13:19:07\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Training...\n",
      "Run name: perc_Hourly_s32776_l6_lMAPE\n",
      "TTime: 2:19:49\n",
      "*************** Start time: 15:38:58\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Training...\n",
      "Run name: perc_Hourly_s34927_l6_lSMAPE\n"
     ]
    }
   ],
   "source": [
    "seasonal_pattern = 'Hourly'\n",
    "\n",
    "#For experiments in to_train\n",
    "for experiment in combinations:\n",
    "    meta_name = experiment[0]\n",
    "    lookback = experiment[1]\n",
    "    loss_name = experiment[2]\n",
    "    count = experiment[3]\n",
    "\n",
    "    print('*************** Start time:', datetime.datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    t.cuda.empty_cache()\n",
    "    seed = np.random.randint(2**16)\n",
    "    t.manual_seed(seed)\n",
    "    trainer = Trainer()\n",
    "    trainer.fit(meta_name, seed, loss_name)\n",
    "\n",
    "    #update tracking\n",
    "    tracking[str(lookback) + '_' + loss_name]['counts'] += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "614b1fd855a2bf271a75ec5d855bec4f3fc12d7e815bbbbf3561ebd1de53ebbb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
